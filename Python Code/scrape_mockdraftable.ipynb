{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The goal of this notebook will be to scrape the combine data contained on (MockDraftable)[https://www.mockdraftable.com].\n",
    "\n",
    "Our first attempt at scraping the data will be to iterate through all position groups, grabbing player information as we go.  The way positions are stored on the site are very interesting.  They basic format is listed below:\n",
    "\n",
    "ATH (Athlete)\n",
    " |\n",
    " |__ SKILL (Skill Position Player)\n",
    " |     |\n",
    " |     |__ QB (Quarterback)\n",
    " |     |\n",
    " |     |__ BALL (Ball Carrier)\n",
    " |           |\n",
    " |           |__ RB (Running Back)\n",
    " |           |    |\n",
    " |           |    |__ FB (Fullback)\n",
    " |           |    |\n",
    " |           |    |__ HB (Halfback)\n",
    " |           |\n",
    " |           |__ WR (Wide Receiver)\n",
    " |           |\n",
    " |           |__ TE (Tight End)\n",
    " |           \n",
    " |__ OL (Offensive Line)\n",
    " |   |\n",
    " |   |__ OT (Offensive Tackle)\n",
    " |   | \n",
    " |   |__ IOL (Interior Offensive Line)\n",
    " |        |\n",
    " |        |__ OG (Offensive Guard)\n",
    " |        |\n",
    " |        |__ OC (Offensive Center)\n",
    " |\n",
    " |__ ST (Special Teams)\n",
    " |\n",
    " |__ DL (Defensive Line)\n",
    " |   |\n",
    " |   |__ IDL (Interior Defensive Line)\n",
    " |   |    |\n",
    " |   |    |__ DT (Defensive Tackle)\n",
    " |   |\n",
    " |   |__ DE (One-Gap Defensive End)\n",
    " |\n",
    " |__ EDGE (Edge Defender)\n",
    " |\n",
    " |__ LB (Linebacker)\n",
    " |   |\n",
    " |   |__ OBLB (Off-Ball Linebacker)\n",
    " |         |\n",
    " |         |__ ILB (Inside Linebacker)\n",
    " |         |\n",
    " |         |__ OLB (Outside Linebacker)\n",
    " |\n",
    " |__ DB (Defensive Back)\n",
    "     |\n",
    "     |__ S (Safety)\n",
    "     |   |\n",
    "     |   |__ SS (Box/Strong Safety)\n",
    "     |   |\n",
    "     |   |__ FS (Deep/Free Safety)\n",
    "     |\n",
    "     |__ CB (Cornerback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advancePage(browser):\n",
    "    # Extract the navigation bar at the bottom of the page for navigation\n",
    "    navBar = browser.find_elements_by_class_name('btn-group')\n",
    "    \n",
    "    # Advance to the next page by hitting the next button\n",
    "    navBar[len(navBar)-1].click()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeURL(position):\n",
    "    #baseURL = 'https://www.mockdraftable.com/search?position=QB&beginYear=1999&endYear=2018&sort=DESC&page='\n",
    "    URL = 'https://www.mockdraftable.com/search?position=' + position + '&beginYear=1999&endYear=2018&sort=DESC&page=1'\n",
    "    return URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageNumberStatus(soup):\n",
    "    selectedButton = soup.find_all('button',{'class':'btn btn-secondary active'})[-1]\n",
    "    lastButton = soup.find_all('button',{'class':'btn btn-secondary'})[-2]\n",
    "    return [int(selectedButton.text), int(lastButton.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievePlayerURL(soup,linkList):\n",
    "    playerLinks = soup.find_all('a', {'class':'list-group-item list-group-item-action justify-content-between d-flex'})\n",
    "    for link in playerLinks:\n",
    "        linkList.append('https://www.mockdraftable.com/' +link['href'])\n",
    "    return linkList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupifyURL(url):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish default header information\n",
    "headers = {\"User-agent\":\n",
    "           \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36\"}\n",
    "\n",
    "# create a list of all the positions we'll be scraping\n",
    "positionList = ['QB','FB','HB','WR','TE','OT','OG','OC','ST','DT','DE','EDGE','ILB','OLB','SS','FS','CB']\n",
    "player_URL_List = []\n",
    "\n",
    "#  Use the instructions found here to install PhantomJS on Ubuntu:\n",
    "#       https://www.vultr.com/docs/how-to-install-phantomjs-on-ubuntu-16-04\n",
    "\n",
    "# Open a PhantomJS web browser and direct it to the DEA's dropbox search page\n",
    "browser = webdriver.PhantomJS()\n",
    "#browser = webdriver.Firefox()\n",
    "browser.implicitly_wait(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QB\n",
      "FB\n",
      "HB\n",
      "WR\n",
      "TE\n",
      "OT\n",
      "OG\n",
      "OC\n",
      "ST\n",
      "DT\n",
      "DE\n",
      "EDGE\n",
      "ILB\n",
      "OLB\n",
      "SS\n",
      "FS\n",
      "CB\n"
     ]
    }
   ],
   "source": [
    "# Iterate through every position we want to scrape\n",
    "for position in positionList:\n",
    "    # make the first version of the URL\n",
    "    url = makeURL(position)\n",
    "    browser.get(url)\n",
    "    pageStatus = [0, 1]    \n",
    "    print(position)\n",
    "    \n",
    "    # Iterate through every subsequent page in the position group\n",
    "    while (pageStatus[0] < pageStatus[1]):\n",
    "        soup = soupifyURL(browser.current_url)\n",
    "        pageStatus = pageNumberStatus(soup)\n",
    "        #print('Current Page: ' + str(pageStatus[0]) + ', Next Page: ' + str(pageStatus[1]))\n",
    "        \n",
    "        player_URL_List = retrievePlayerURL(soup, player_URL_List)\n",
    "\n",
    "        # advance the page\n",
    "        advancePage(browser)\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_URL_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievePlayerInfo(soup):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in player_URL_List:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
